In the Classification download, the user's responses to each task are recorded in a field called 'annotations' (including those of experts and those tagged "TRUE" for being gold standard responses. The field is a JSON string. When loaded into Python using the json.loads() function from import json, it becomes a dictionary of dictionaries of form {{....}, {.....}, {....}} with a dictionary for each task the user is required to complete, in the order completed. The form of the individual dictionaries for the tasks depends very much on what type of task it is - a question, drawing tool, transcription or survey task.  This set of scripts is for question tasks, which have the form {"task":"TX", "task_label":"question the users saw", "value":"answers chosen"}.

For simple questions with one answer choice this is pretty simple, especially if one knows which of the blocks in annotations we are dealing with. Even without that we can find the correct task because we know the question text.  Due to the way the project builder can be assembled, it is unlikely tasks are numbered in the order completed, and with the conditional branching allowed, tasks are not even in the same order in every response.

The following blocks describe ways to address these issues for question tasks. To use these, the correct block for the project structure must be chosen, it must be integrated into the basic flatten_class_frame.py following the instructions in the comments for each block, and appropriate snippets of the question used in your project copied and pasted from your project tasks to the code.  Note the snippets are case sensitive and spaces and punctuation count – it is best to copy and paste to be sure of the match.  In general the order the blocks are added does not matter as they are independent of each other.  The order and field names in the output file definition and the writer must match, though they can be any names you want.  The general utilities blocks can also added as shown in the demo.

#### Question with only single answers (radio button questions)

Block 1 -  Question task is the first task completed, and has a single REQUIRED answer.
The simplest case since we can determine the task number directly and we know it will be present!  The block returns the exact answer chosen as a text string.  We can then modify this in many ways – a few useful examples are given.

Block 2 - Question task is not necessarily the first task completed, and has a single answer which may not be required:  Output is the actual answer, one column per question.  Note in this case we do not know where or if the associated task will appear in annotations.  We use exact snippets of the actual questions to locate the answers chosen. The answers can be modified or even replaced with other more useful values as in the examples in Block 1. (This block can handle first questions as well).

#### Multiple-allowed answer questions: 

While Block 2 can handle multi-answer questions, the multiple answers will be in a list with more than one string or value in it.  To break out multiple answers we can either accept this and worry about simplifying the field later, we can split the multiple answers out into separate columns (Block 3),  or we can generate a answer vector which contains all the information but in a simpler form than a list of answer texts using the verbiage from the project. One possible answer vector using integers which allow easy aggregation counts of each answer is presented in Block 4

Block 3 -  Questions with multiple-allowed answers split out into separate columns for each possible answer.  
The block uses exact snippets of the actual question to find the list of answers chosen for that task, then uses snippets of the answers to assign each answer to the correct field variable.  The snippet of question must match the question asked in **only one task**, and must be sufficiently unique that it is not found in any other task label.  The answer snippets must match part of **only one answer for that task** – they may match answers in other tasks  - ie  ‘Yes’ may safely appear as answers to questions in more than one task.  The block assumes there will be a separate field names/column set up for every possible answer to the questions being queried.  This rapidly becomes unwieldy if there are many questions and answers – this is addressed in Block 4.

Block 4  Multiple-allowed answers output in a answer vector.
An answer vector is an ordered list of N entries each of which indexable ie the choice for each possible answer is in a prescribed location in the list.  Each entry in the list is a flag set to indicate one of two possible states for each option in the N multiple-allowed answers.  In the simplest case the flags are 0 and 1 for the choice ‘not selected’ or ‘selected’, though any two distinct values could be used. For example this answer vector for a question with five possible choice answers  [0, 1, 0, 0, 1] could mean choices 2 and 5 were selected.

The advantage of using answer vectors is the flags can be integers which makes aggregation very easy. The downside is this sort of output is distanced from the answer text and one needs to know what answer each element is associated with, particularly if the flags used simply 0 and 1's.

To generate the answer vector we introduce the answer template - this is a list with one to one correspondence to the answer vector with each element a unique snippet of the answer choice which is found **only** within the appropriate answer choice and **no where else** in the json string for that task's values.  Based on the details of the project we define the answer template, and then search systematically through the text of the task values for each element.  If found in the task values we set the corresponding flag to indicate that answer was one of those chosen, otherwise it is left as the unselected flag.

In the output file there is one column per multiple-allowed answer question which contains the answer vector for that record.  Unlike the list of answers returned by block 2, the answer vector always has an element for each possible answer – either as selected or not selected.  If the task is skipped or not required in the workflow, or no answers are selected it shows all elements as unselected. An optional test for this condition can be used to return an empty field if the task is skipped and a empty list if it is performed but no answers are chosen. 

####  Question Demo - flatten_class_question_demo.py

This file uses Block 4 to flatten a task question for the original FossilFinder workflow 371 (workflow_versions > 160.01) which had five possible answers.  While only one answer was allowed, I have used the multiple-allowed answer vector block to reduce the list of possible answers in the JSON format to a list of 0, 1’s where 1 is the answer selected.  Note the task_answer_template_1 which holds the answer snippets.
The demo then goes on to flatten two further simple question tasks into separate columns using Block 2. It also uses one of the general utilities blocks and an external user_ip:assigned_name file to replace not-logged-in user names.  The input file is again selected records from Fossil Finder - fossil-finder-classifications_test.csv, and the output file is flatten_class_questions_demo.csv.

